# -*- coding: utf-8 -*-
"""foursquare

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17nehZ0g-ndi2JFUKZ9psS37w4YOYYrUY

Librerías básicas (lecturas, gráficas, numéricas)
"""

import pandas as pd
import numpy as np 
import matplotlib.pyplot as plt 
import seaborn as sns

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

# general
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt


# modelling
from sklearn.preprocessing import StandardScaler, MinMaxScaler 
from sklearn.model_selection import train_test_split, cross_val_score 
from sklearn.linear_model import LinearRegression
from sklearn import linear_model
from sklearn import metrics
from sklearn import preprocessing 
import xgboost as xgb
from xgboost import plot_importance
from sklearn.metrics import explained_variance_score, mean_squared_error, r2_score

#Hide warnings
import warnings
warnings.filterwarnings('ignore')

# Set plot preference
plt.style.use(style='ggplot')
plt.rcParams['figure.figsize'] = (10, 6)
# %matplotlib inline

print('Libraries imported.')

pip show matplotlib



from google.colab import files
uploaded = files.upload()

"""Lectura de datos"""

dataset=pd.read_csv('data_part1.csv', index_col=0)

dataset.head()

print(f'The number of rows are {dataset.shape[0]} \nand the number of columns are {dataset.shape[1]}')

print(dataset.dtypes)

dataset.describe()

dataset.cov()

"""Correlaciones entre variables, relevante para DEA (tienen que tener alta correlación)"""

dataset.corr()

sns.pairplot(dataset,diag_kind='kde')

"""Preparación de los Datos"""

dataset.isna().sum()

def missing(df):
    total=df.isnull().sum().sort_values(ascending=False)
    percent=(df.isnull().sum()*100/df.isnull().count()).sort_values(ascending=False)

missing(dataset)

dataset.isnull().sum()

features=list(dataset.select_dtypes(exclude=['object']))
fig=plt.subplots(figsize=(15,30))
for i, j in enumerate(features):
    plt.subplot(6, 2, i+1),
    plt.subplots_adjust(hspace = 1.0)
    sns.boxplot(dataset[j])
    plt.title(j)

cora=dataset.corr()

"""Para DEA buscamos la de mayor correlación """

plt.figure(figsize=(15,10))
sns.heatmap(cora,annot=True)

pca_comp

cor=pca_comp.corr()

plt.figure(figsize=(15,10))
sns.heatmap(cor,annot=True)

sns.pairplot(pca_comp,diag_kind='kde')

features=list(pca_comp.select_dtypes(exclude=['object']))
fig=plt.subplots(figsize=(15,30))
for i, j in enumerate(features):
    plt.subplot(6, 2, i+1),
    plt.subplots_adjust(hspace = 1.0)
    sns.distplot(pca_comp[j])
    plt.title(j)

features=list(pca_comp.select_dtypes(exclude=['object']))
fig=plt.subplots(figsize=(15,30))
for i, j in enumerate(features):
    plt.subplot(6, 2, i+1),
    plt.subplots_adjust(hspace = 1.0)
    sns.boxplot(pca_comp[j])
    plt.title(j)

from sklearn.cluster import KMeans

cluster_range = range( 1, 15 )
cluster_errors = []

for num_clusters in cluster_range:
  clusters = KMeans( num_clusters,n_init = 15, random_state=2)
  clusters.fit(pca_comp)
  labels = clusters.labels_
  centroids = clusters.cluster_centers_
  cluster_errors.append( clusters.inertia_ )
clusters_df = pd.DataFrame( { "num_clusters":cluster_range, "cluster_errors": cluster_errors } )
clusters_df[0:15]

plt.figure(figsize=(12,6))
plt.plot( clusters_df.num_clusters, clusters_df.cluster_errors, marker = "o" )

kmeans = KMeans(n_clusters=4, n_init = 15, random_state=2)
kmeans.fit(X_pca)

centroids=kmeans.cluster_centers_


# creating a new dataframe only for labels and converting it into categorical variable
df_labels = pd.DataFrame(kmeans.labels_ , columns = list(['labels']))

df_labels['labels'] = df_labels['labels'].astype('category')

kmeans = KMeans(n_clusters=8,random_state=42)
clusters = kmeans.fit_predict(pca_comp)
df_k = country.copy(deep=True)
df_k['label'] = clusters

df_k

from sklearn.cluster import AgglomerativeClustering
model2 = AgglomerativeClustering(n_clusters=8, affinity='euclidean',  linkage='ward')
model2.fit(X_pca)

from scipy.cluster.hierarchy import dendrogram, linkage
from scipy.spatial.distance import pdist

plt.figure(figsize=(18, 16))
plt.title('Agglomerative Hierarchical Clustering Dendogram')
plt.xlabel('sample index')
plt.ylabel('Distance')
Z = linkage(country_scaled, 'ward')
dendrogram(Z,leaf_rotation=90.0,p=5,color_threshold=70,leaf_font_size=10,truncate_mode='level')
plt.tight_layout()

clusters_df = pd.DataFrame( { "num_clusters":cluster_range, "Inertia": cluster_errors } )
clusters_df[0:15]

# Commented out IPython magic to ensure Python compatibility.
from __future__ import print_function
# %matplotlib inline


from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_samples, silhouette_score

import matplotlib.pyplot as plt
import matplotlib.cm as cm
import numpy as np

print(__doc__)

X=X_pca

range_n_clusters = [2, 3, 4, 5,6,7,8]

for n_clusters in range_n_clusters:
    fig, (ax1, ax2) = plt.subplots(1, 2)
    fig.set_size_inches(18, 7)

    ax1.set_xlim([-0.1, 1])
    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])

    clusterer = KMeans(n_clusters=n_clusters,n_init=15, random_state=2)
    cluster_labels = clusterer.fit_predict(X)

    silhouette_avg = silhouette_score(X, cluster_labels)
    print("For n_clusters =", n_clusters,
          "The average silhouette_score is :", silhouette_avg)

    sample_silhouette_values = silhouette_samples(X, cluster_labels)

    y_lower = 10
    for i in range(n_clusters):
        # Aggregate the silhouette scores for samples belonging to
        # cluster i, and sort them
        ith_cluster_silhouette_values = \
            sample_silhouette_values[cluster_labels == i]

        ith_cluster_silhouette_values.sort()

        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i

        color = cm.Spectral(float(i) / n_clusters)
        ax1.fill_betweenx(np.arange(y_lower, y_upper),
                          0, ith_cluster_silhouette_values,
                          facecolor=color, edgecolor=color, alpha=0.7)

        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

        y_lower = y_upper + 10  

    ax1.set_title("The silhouette plot for the various clusters.")
    ax1.set_xlabel("The silhouette coefficient values")
    ax1.set_ylabel("Cluster label")

    ax1.axvline(x=silhouette_avg, color="red", linestyle="--")

    ax1.set_yticks([])  
    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])

    colors = cm.Spectral(cluster_labels.astype(float) / n_clusters)
    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,
                c=colors)

    centers = clusterer.cluster_centers_
    ax2.scatter(centers[:, 0], centers[:, 1],
                marker='o', c="white", alpha=1, s=200)

    for i, c in enumerate(centers):
        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)

    ax2.set_title("The visualization of the clustered data.")
    ax2.set_xlabel("Feature space for the 1st feature")
    ax2.set_ylabel("Feature space for the 2nd feature")

    plt.suptitle(("Silhouette analysis for KMeans clustering on sample data "
                  "with n_clusters = %d" % n_clusters),
                 fontsize=14, fontweight='bold')

    plt.show()

df_k[df_k.loc[:,'label']==0].mean()

df_k[df_k.loc[:,'label']==1].mean()

df_k[df_k.loc[:,'label']==2].mean()

df_k[df_k.loc[:,'label']==0].index

df_k[df_k.loc[:,'label']==1].index

df_k[df_k.loc[:,'label']==2].index

df_k[df_k.loc[:,'label']==3].index

df_k[df_k.loc[:,'label']==4].index

df_k[df_k.loc[:,'label']==5].index

print(list(df_k[df_k.loc[:,'label']==6].index))

print(list(df_k[df_k.loc[:,'label']==7].index))

print(df_k[df_k.loc[:,'label']==1].mean())
print(df_k[df_k.loc[:,'label']==0].mean())

import sys
!{sys.executable} -m pip install geopandas==0.6.3
!{sys.executable} -m pip install mplleaflet
!{sys.executable} -m pip install bs4
!{sys.executable} -m pip install geocoder
!{sys.executable} -m pip install geopy
!{sys.executable} -m pip install folium
!{sys.executable} -m pip install lxml
!{sys.executable} -m pip install pygeoj
!{sys.executable} -m pip install pyshp
!{sys.executable} -m pip install datetime
!{sys.executable} -m pip install seaborn
!{sys.executable} -m pip install --upgrade cython

# required packages for neighbourhood analysis
!{sys.executable} -m pip install descartes
!{sys.executable} -m pip install requests

!{sys.executable} -m pip install pandana

# required packages for modelling
!{sys.executable} -m pip install xgboost

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

# general
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Geographical analysis
import geopandas as gpf #libspatialindex nees to be installed first
import json # library to handle JSON files
from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe
import shapefile as shp
import datetime
from statsmodels.tsa.seasonal import seasonal_decompose
import requests
import descartes

# accessibility analysis
import time
from pandana.loaders import osm
from pandana.loaders import pandash5

# modelling
from sklearn.preprocessing import StandardScaler, MinMaxScaler 
from sklearn.model_selection import train_test_split, cross_val_score 
from sklearn.linear_model import LinearRegression
from sklearn import linear_model
from sklearn import metrics
from sklearn import preprocessing 
import xgboost as xgb
from xgboost import plot_importance
from sklearn.metrics import explained_variance_score, mean_squared_error, r2_score

#Hide warnings
import warnings
warnings.filterwarnings('ignore')

# Set plot preference
plt.style.use(style='ggplot')
plt.rcParams['figure.figsize'] = (10, 6)
# %matplotlib inline

print('Libraries imported.')

country.head()

def multi_collinearity_heatmap(df, figsize=(11,9)):
    
    """
    Creates a heatmap of correlations between features in the df. A figure size can optionally be set.
    """
    
    # Set the style of the visualization
    sns.set(style="white")

    # Create a covariance matrix
    corr = country.corr()

    # Generate a mask the size of our covariance matrix
    mask = np.zeros_like(corr, dtype=np.bool)
    mask[np.triu_indices_from(mask)] = True

    # Set up the matplotlib figure
    f, ax = plt.subplots(figsize=figsize)

    # Generate a custom diverging colormap
    cmap = sns.diverging_palette(220, 10, as_cmap=True)

    # Draw the heatmap with the mask and correct aspect ratio
    sns.heatmap(corr, mask=mask, cmap=cmap, center=0, square=True, linewidths=.5, cbar_kws={"shrink": .5}, vmax=corr[corr != 1.0].max().max());

multi_collinearity_heatmap(country, figsize=(20,20))

X = country.drop('valor', axis=1)
y = country.valor

import numpy as np 
import matplotlib as mpl 
from mpl_toolkits.mplot3d import Axes3D 
import matplotlib.pyplot as plt 
  
def generate_dataset(n): 
    x = [] 
    y = [] 
    random_x1 = np.random.rand() 
    random_x2 = np.random.rand() 
    for i in range(n): 
        x1 = i 
        x2 = i/2 + np.random.rand()*n 
        x.append([1, x1, x2]) 
        y.append(random_x1 * x1 + random_x2 * x2 + 1) 
    return np.array(x), np.array(y)

x, y = generate_dataset(136) 
  
mpl.rcParams['legend.fontsize'] = 12
  
fig = plt.figure() 
ax = fig.gca(projection ='3d') 
  
ax.scatter(x[:, 1], x[:, 2], y, label ='y', s = 5) 
ax.legend() 
ax.view_init(45, 0) 
  
plt.show()

def mse(coef, x, y): 
    return np.mean((np.dot(x, coef) - y)**2)/2
  
def gradients(coef, x, y): 
    return np.mean(x.transpose()*(np.dot(x, coef) - y), axis = 1) 
  
def multilinear_regression(coef, x, y, lr, b1 = 0.9, b2 = 0.999, epsilon = 1e-8): 
    prev_error = 0
    m_coef = np.zeros(coef.shape) 
    v_coef = np.zeros(coef.shape) 
    moment_m_coef = np.zeros(coef.shape) 
    moment_v_coef = np.zeros(coef.shape) 
    t = 0
  
    while True: 
        error = mse(coef, x, y) 
        if abs(error - prev_error) <= epsilon: 
            break
        prev_error = error 
        grad = gradients(coef, x, y) 
        t += 1
        m_coef = b1 * m_coef + (1-b1)*grad 
        v_coef = b2 * v_coef + (1-b2)*grad**2
        moment_m_coef = m_coef / (1-b1**t) 
        moment_v_coef = v_coef / (1-b2**t) 
  
        delta = ((lr / moment_v_coef**0.5 + 1e-8) * 
                 (b1 * moment_m_coef + (1-b1)*grad/(1-b1**t))) 
  
        coef = np.subtract(coef, delta) 
    return coef 
  
coef = np.array([0, 0, 0]) 
c = multilinear_regression(coef, x, y, 1e-1) 
fig = plt.figure() 
ax = fig.gca(projection ='3d') 
  
ax.scatter(x[:, 1], x[:, 2], y, label ='y', 
                s = 5, color ="dodgerblue") 
  
ax.scatter(x[:, 1], x[:, 2], c[0] + c[1]*x[:, 1] + c[2]*x[:, 2], 
                    label ='regression', s = 5, color ="orange") 
  
ax.view_init(45, 0) 
ax.legend() 
plt.show()

import pandas as pd
import numpy as np
import itertools
from itertools import chain, combinations
import statsmodels.formula.api as smf
import scipy.stats as scipystats
import statsmodels.api as sm
import statsmodels.stats.stattools as stools
import statsmodels.stats as stats
from statsmodels.graphics.regressionplots import *
import matplotlib.pyplot as plt
import seaborn as sns
import copy
import math
import time

# Splitting into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)

hpm_reg_start = time.time()

# Create instance of the model, `LinearRegression` function from 
# Scikit-Learn and fit the model on the training data:

hpm_reg = LinearRegression()  
hpm_reg.fit(X_train, y_train) #training the algorithm

# Now that the model has been fit we can make predictions by calling 
# the predict command. We are making predictions on the testing set:
training_preds_hpm_reg = hpm_reg.predict(X_train)
val_preds_hpm_reg = hpm_reg.predict(X_test)

hpm_reg_end = time.time()

print(f"Time taken to run: {round((hpm_reg_end - hpm_reg_start)/60,1)} minutes")

# Check the predictions against the actual values by using the MSE and R-2 metrics:
print("\nTraining RMSE:", round(mean_squared_error(y_train, training_preds_hpm_reg),4))
print("Validation RMSE:", round(mean_squared_error(y_test, val_preds_hpm_reg),4))
print("\nTraining r2:", round(r2_score(y_train, training_preds_hpm_reg),4))
print("Validation r2:", round(r2_score(y_test, val_preds_hpm_reg),4))

y_test_array = np.array(list(y_test))
val_preds_hpm_reg_array = np.array(val_preds_hpm_reg)
hpm_df = pd.DataFrame({'Actual': y_test_array.flatten(), 'Predicted': val_preds_hpm_reg_array.flatten()})
hpm_df

actual_values = y_test
plt.scatter(val_preds_hpm_reg, actual_values, alpha=.7,
            color='r') #alpha helps to show overlapping data
overlay = 'R^2 is: {}\nRMSE is: {}'.format(
                    (round(r2_score(y_test, val_preds_hpm_reg),4)),
                    (round(mean_squared_error(y_test, val_preds_hpm_reg))),4)
plt.annotate( s=overlay,xy=(5.5,2.5),size='x-large')
plt.xlabel('Predicted Imports')
plt.ylabel('Actual Imports')
plt.title('SHP Regression Model')
plt.show()

xgb_reg_start = time.time()

xgb_reg = xgb.XGBRegressor()
xgb_reg.fit(X_train, y_train)
training_preds_xgb_reg = xgb_reg.predict(X_train)
val_preds_xgb_reg = xgb_reg.predict(X_test)

xgb_reg_end = time.time()

print(f"Time taken to run: {round((xgb_reg_end - xgb_reg_start)/60,1)} minutes")
print("\nTraining MSE:", round(mean_squared_error(y_train, training_preds_xgb_reg),4))
print("Validation MSE:", round(mean_squared_error(y_test, val_preds_xgb_reg),4))
print("\nTraining r2:", round(r2_score(y_train, training_preds_xgb_reg),4))
print("Validation r2:", round(r2_score(y_test, val_preds_xgb_reg),4))

ft_weights_xgb_reg = pd.DataFrame(xgb_reg.feature_importances_, columns=['weight'], index=X_train.columns)
ft_weights_xgb_reg.sort_values('weight', ascending=False, inplace=True)
ft_weights_xgb_reg.head(13)

# Plotting feature importances
plt.figure(figsize=(10,25))
plt.barh(ft_weights_xgb_reg.index, ft_weights_xgb_reg.weight, align='center') 
plt.title("Feature importances in the XGBoost model", fontsize=14)
plt.xlabel("Feature importance")
plt.margins(y=0.01)
plt.show()

"""Para regresión no sirve, el valor medio es mejor que la regresión (R2 negativo) pero tampoco obedecemos el número de observaciones necesarias para el número de variables independientes (tenemos 12 variables y el número sería 50+8n por lo que debería ser 146 observaciones y tenemos 136 al hacer la limpieza).
para ver si un predictor específico tiene influencia cuadramos porque  104 + n o sea 116 observaciones. 
"""

pip install pyDEA

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pyDEA as dea
import pandas as pd

# %matplotlib inline

input_cols = [u'E01 Balanza comercial', u'E02 Precio por tonelada', u'E03 Crecimiento valor 5 años', u'E04 Crecimiento cantidades 5 añis', u'E05 Crecimiento ultimo año valor', u'E06 Ranking importaciones mundiales', u'E07 Porcentaje importaciones mundiales', u'E08 crecimiento importaciones mundial', u'E09 Distancia de España', u'E10 Concentración de los 50 exportadores', u'E11 Tarifa promedio']
output_cols = ['valor', 'Cantidad exportadas', ]
inputs=

import math

sns.set_style("white")

num_plots = len(input_cols)
n = int(math.ceil(math.sqrt(num_plots)))

fig = plt.figure(figsize=(20, 20))
axes = [plt.subplot(n, n, i) for i in range(1, num_plots + 1)]

i = 0
for k, v in inoutdat[input_cols].iteritems():
    ax = axes[i]
    sns.kdeplot(v, shade=True, ax=ax, legend=False)
    [label.set_visible(False) for label in ax.get_yticklabels()]
    ax.xaxis.set_ticks([v.min(), v.max()])
    ax.set_title(k)
    i += 1
sns.despine(left=True, trim=True, fig=fig)
plt.tight_layout()

country.head()

