# -*- coding: utf-8 -*-
"""Scrapy albania

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pt0Kdha8s6qCwURI0QmIqWUnmEKGuk3m
"""

# paquetes generales
# %load basic_installs.py
import sys
!{sys.executable} -m pip install geopandas==0.6.3
!{sys.executable} -m pip install mplleaflet
!{sys.executable} -m pip install bs4
!{sys.executable} -m pip install geocoder
!{sys.executable} -m pip install geopy
!{sys.executable} -m pip install folium
!{sys.executable} -m pip install lxml
!{sys.executable} -m pip install pygeoj
!{sys.executable} -m pip install pyshp
!{sys.executable} -m pip install datetime
!{sys.executable} -m pip install seaborn
!{sys.executable} -m pip install --upgrade cython

# para series temporales
!{sys.executable} -m pip install statsmodels

# required packages for neighbourhood analysis
!{sys.executable} -m pip install descartes
!{sys.executable} -m pip install requests

# requiered packages for accessibility analysis
# Make sure Cython is upgraded FIRST!
!{sys.executable} -m pip install pandana

# required packages for modelling
!{sys.executable} -m pip install xgboost

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

# general
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Geographical analysis
import geopandas as gpf #libspatialindex nees to be installed first
import json # library to handle JSON files
from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe
import shapefile as shp
import datetime
from statsmodels.tsa.seasonal import seasonal_decompose
import requests
import descartes

# accessibility analysis
import time
from pandana.loaders import osm
from pandana.loaders import pandash5

# modelling
from sklearn.preprocessing import StandardScaler, MinMaxScaler 
from sklearn.model_selection import train_test_split, cross_val_score 
from sklearn.linear_model import LinearRegression
from sklearn import linear_model
from sklearn import metrics
from sklearn import preprocessing 
import xgboost as xgb
from xgboost import plot_importance
from sklearn.metrics import explained_variance_score, mean_squared_error, r2_score

#Hide warnings
import warnings
warnings.filterwarnings('ignore')

# Set plot preference
plt.style.use(style='ggplot')
plt.rcParams['figure.figsize'] = (10, 6)
# %matplotlib inline

print('Libraries imported.')

pip install xlrd

from google.colab import drive 
drive.mount('/content/gdrive')

from google.colab import files
uploaded = files.upload()

#!pwd to check the working directory
raw_df = pd.read_csv('listings.csv')
print(f"The dataset contains {len(raw_df)} Airbnb listings")
pd.set_option('display.max_columns', len(raw_df.columns)) # To view all columns
pd.set_option('display.max_rows', 100)
raw_df.head(3)

df.describe()

from pandas.io.json import json_normalize
import folium
from geopy.geocoders import Nominatim 
import requests

# Define Foursquare Credentials 

CLIENT_ID = "TN5TVCGWYYAUSKIHBASFJW1DYKSIWE14OQDSJKOD2OXG2GXV"
CLIENT_SECRET = "ICVRSAJCMQBGYDW0CIPW2VR1SZCEQXSKRKGIISTO0L3IAJZZ"
VERSION = '20190425'
LIMIT = 30
print('Your credentails:')
print('CLIENT_ID: ' + CLIENT_ID)
print('CLIENT_SECRET:' + CLIENT_SECRET)

map_df = pd.DataFrame(columns=['latitude', 'longitude','neighbourhood'])
map_df.neighbourhood=map_df.neighbourhood.astype(str)
map_df.head()

# Function to loop for venues through all neighbourhoods
#url = 'https://api.foursquare.com/v2/venues/search?categoryId=50aa9e094b90af0d42d5de0d,530e33ccbcbc57f1066bbff3,530e33ccbcbc57f1066bbff9,4f2a25ac4b909258e854f55f&intent=browse&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format()
def getNearbyVenues(names, latitudes, longitudes, radius):
    venues_list=[]
    count = 0
    for name, lat, lng in zip(names, latitudes, longitudes):
        #print(name)
        
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)
        
        try:
          # make the GET request
          results = requests.get(url).json()["response"]['groups'][0]['items']
          #print(results)

          # return only relevant information for each nearby venue
          venues_list.append([(
              name, 
              lat, 
              lng, 
              v['venue']['name'], 
              v['venue']['location']['lat'], 
              v['venue']['location']['lng'],  
              v['venue']['categories'][0]['name']) for v in results])
        except:
            pass
        count = count+1
        if count == 700:
            break
        
    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighbourhood', 
                  'Neighbourhood Latitude', 
                  'Neighbourhood Longitude', 
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Venue Category']
    
    return(nearby_venues)

albania = getNearbyVenues(names=map_df['neighbourhood'],
                                   latitudes=map_df['latitude'],
                                   longitudes=map_df['longitude'],
                                   radius = 150
                                  )

"""Checking the size of the returning dataframe"""

print(edinburgh_venues.shape)
edinburgh_venues.head()

# Saving Data set
edinburgh_venues.to_csv(r'Edinburgh_Venues.csv')

# Read dataset
edinburgh_venues = pd.read_csv('Edinburgh_Venues.csv', index_col=0)

"""Venues returned per neighbourhood"""

edinburgh_venues.groupby('Neighbourhood').count()

"""**Unique categories of venues**"""

print('There are {} unique categories.'.format(len(edinburgh_venues['Venue Category'].unique())))

"""**Number of venues per category**"""

edinburgh_venues.groupby('Venue Category').count()

"""**Analysis per neighbourhood**"""

# One Hot Encoding
edinburgh_onehot = pd.get_dummies(edinburgh_venues[['Venue Category']], prefix = "", prefix_sep = "")

## Add neighbourhood column back to df
edinburgh_onehot['Neighbourhood'] = edinburgh_venues['Neighbourhood']

# Move neighbourhood column to the first column
fixed_columns = [edinburgh_onehot.columns[-1]] + list(edinburgh_onehot.columns[:-1])
edinburgh_onehot = edinburgh_onehot[fixed_columns]

# display
edinburgh_onehot.head()

# New df dimensions
edinburgh_onehot.shape

"""**Group rows by neighbourhood and by taking the mean and the frequency of occurrence of each category**"""

edinburgh_grouped = edinburgh_onehot.groupby('Neighbourhood').mean().reset_index()
edinburgh_grouped

"""**Get each neighbourhood along with its top 5 most common venues**"""

num_top_venues = 5

for hood in edinburgh_grouped['Neighbourhood']:
    print("----"+hood+"----")
    temp = edinburgh_grouped[edinburgh_grouped['Neighbourhood'] == hood].T.reset_index()
    temp.columns = ['venue','freq']
    temp = temp.iloc[1:]
    temp['freq'] = temp['freq'].astype(float)
    temp = temp.round({'freq': 2})
    print(temp.sort_values('freq', ascending=False).reset_index(drop=True).head(num_top_venues))
    print('\n')

"""**Put that in pandas data frame**"""

# Function to sort venues in descending order
def return_most_common_venues(row, num_top_venues):
    row_categories = row.iloc[1:]
    row_categories_sorted = row_categories.sort_values(ascending=False)
    
    return row_categories_sorted.index.values[0:num_top_venues]

import numpy as np

# New dataframe ordered
indicators = ['st', 'nd', 'rd']

# create columns according to number of top venues
columns = ['Neighbourhood']
for ind in np.arange(num_top_venues):
    try:
        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))
    except:
        columns.append('{}th Most Common Venue'.format(ind+1))

# create a new dataframe
neighbourhoods_venues_sorted = pd.DataFrame(columns=columns)
neighbourhoods_venues_sorted['Neighbourhood'] = edinburgh_grouped['Neighbourhood']

for ind in np.arange(edinburgh_grouped.shape[0]):
    neighbourhoods_venues_sorted.iloc[ind, 1:] = return_most_common_venues(edinburgh_grouped.iloc[ind, :], 
                                                                          num_top_venues)

neighbourhoods_venues_sorted.head()

"""**Let's look at how many categories are on the 1st most common venue.**"""

print('There are {} unique categories.'.format(len(neighbourhoods_venues_sorted['1st Most Common Venue'].unique())))

neighbourhoods_venues_sorted.groupby('1st Most Common Venue').count()

"""From above, we observe that the most common venues across the dataset are Hotel, Pub, Grocery Store, Supermarket, Cafe, Coffee Shop, with Bar, Bus Stop and Indian Restaurant coming behind. It is clear that different restaurant venues are in subcategories which makes them less common than if they were aggregated. Thus, for the purpose accounting for the venues that may have the most impact on price, we will limit the venues to the most common categories.  It is unlikely that having a hotel nearby will affect price, as Airbnb listings are considered to be on a different category of short-term rental, due to the different benefits they provide versus hotels. Thus, that category of venues will not be considered.

### 5. Preparando los datos
"""

# Open dataset for modelling
df = pd.read_csv(r'listings.csv', index_col=0)
df.head()

columns = ['host_name', 'neighbourhood_group', 'neighbourhood',
       'latitude', 'longitude', 'room_type', 'price', 'minimum_nights',
       'number_of_reviews', 'last_review', 'reviews_per_month',
       'calculated_host_listings_count', 'availability_365']
df = df[columns]

cols_to_encode = []
for col in columns:
  if df[col].dtype == 'object':
    cols_to_encode.append(col)

from sklearn.preprocessing import LabelEncoder
for col in cols_to_encode:
  le = LabelEncoder()
  df[col] = df[col].astype(str)
  df[col] = le.fit_transform(df[col])

pip install peewee

pip install settings

from peewee import BooleanField
from peewee import CharField
from peewee import DateField
from peewee import DecimalField
from peewee import ForeignKeyField
from peewee import IntegerField
from peewee import Model

class Coordinate(Model):
    coordinate = CharField()
    address = CharField()


class Venue(Model):
    coordinate = ForeignKeyField(Coordinate, related_name='venues')
    venue_id = CharField()
  


class VenueDetail(Model):
    venue = ForeignKeyField(Venue, related_name="detail")
    name = CharField()
    category = CharField()
    rating =  DecimalField()
    rating_signals = IntegerField()
    checkins_count = IntegerField()
    users_count = IntegerField()
    tip_count = IntegerField()
    visit_count = IntegerField()
    likes = IntegerField()
    photos = IntegerField()
    created_at = DateField()
    listed = IntegerField()
    price = IntegerField()
    verified = BooleanField()

from settings import fs_client

print("Auth URL: {}".format(fs_client.oauth.auth_url()))

code = input("Enter the code from URL: ")
access_token = fs_client.oauth.get_token(code)
fs_client.set_access_token(access_token)